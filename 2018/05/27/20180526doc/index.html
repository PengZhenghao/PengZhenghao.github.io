<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />





  <meta name="msvalidate.01" content="true" />






  <meta name="baidu-site-verification" content="yctzz3arGz" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="前言目前看到的许多文章，都可以认为是一个包含以下部分的框架：  帧采样机制 时段propose机制 位置propose机制 Frame Embedding Video Embedding 分类机制  帧采样机制有许多做法：1. 在整个视频中均匀地采样给定数目的帧。（Weakly2018）2.  时段提名机制有许多，可以分为两类：一、前置类。在监督学习中，有的是使用actioness作为判断依据，再">
<meta property="og:type" content="article">
<meta property="og:title" content="动作检测（Action Detection）文献综述-持续更新">
<meta property="og:url" content="http://PengZhenghao.github.io/2018/05/27/20180526doc/index.html">
<meta property="og:site_name" content="彭正皓的博客">
<meta property="og:description" content="前言目前看到的许多文章，都可以认为是一个包含以下部分的框架：  帧采样机制 时段propose机制 位置propose机制 Frame Embedding Video Embedding 分类机制  帧采样机制有许多做法：1. 在整个视频中均匀地采样给定数目的帧。（Weakly2018）2.  时段提名机制有许多，可以分为两类：一、前置类。在监督学习中，有的是使用actioness作为判断依据，再">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://pengzhenghao.github.io/Users/pengzhenghao/Documents/hexo/source/images/20180526/weakly20182.png">
<meta property="og:image" content="http://pengzhenghao.github.io/Users/pengzhenghao/Documents/hexo/source/images/20180526/weakly2018.png">
<meta property="og:image" content="http://pengzhenghao.github.io/Users/pengzhenghao/Documents/hexo/source/images/20180526/domain2016.png">
<meta property="og:image" content="http://pengzhenghao.github.io/Users/pengzhenghao/Documents/hexo/source/images/20180526/semi2015.png">
<meta property="og:updated_time" content="2018-06-02T10:29:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="动作检测（Action Detection）文献综述-持续更新">
<meta name="twitter:description" content="前言目前看到的许多文章，都可以认为是一个包含以下部分的框架：  帧采样机制 时段propose机制 位置propose机制 Frame Embedding Video Embedding 分类机制  帧采样机制有许多做法：1. 在整个视频中均匀地采样给定数目的帧。（Weakly2018）2.  时段提名机制有许多，可以分为两类：一、前置类。在监督学习中，有的是使用actioness作为判断依据，再">
<meta name="twitter:image" content="http://pengzhenghao.github.io/Users/pengzhenghao/Documents/hexo/source/images/20180526/weakly20182.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://PengZhenghao.github.io/2018/05/27/20180526doc/"/>





  <title>动作检测（Action Detection）文献综述-持续更新 | 彭正皓的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">彭正皓的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">PENG Zhenghao's blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页 | Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类 | Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-随笔-| essay">
          <a href="/categories/随笔/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-coffee"></i> <br />
            
            随笔 | Essay
          </a>
        </li>
      
        
        <li class="menu-item menu-item-技术-| tech">
          <a href="/categories/技术/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-code"></i> <br />
            
            技术 | Tech
          </a>
        </li>
      
        
        <li class="menu-item menu-item-日志-| doc">
          <a href="/categories/日志/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-cube"></i> <br />
            
            日志 | Doc
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-address-card"></i> <br />
            
            关于 | About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://PengZhenghao.github.io/2018/05/27/20180526doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彭正皓">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彭正皓的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">动作检测（Action Detection）文献综述-持续更新</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T15:41:25+08:00">
                2018-05-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/日志/" itemprop="url" rel="index">
                    <span itemprop="name">日志</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前看到的许多文章，都可以认为是一个包含以下部分的框架：</p>
<ul>
<li>帧采样机制</li>
<li>时段propose机制</li>
<li>位置propose机制</li>
<li>Frame Embedding</li>
<li>Video Embedding</li>
<li>分类机制</li>
</ul>
<p>帧采样机制有许多做法：1. 在整个视频中均匀地采样给定数目的帧。（Weakly2018）2. </p>
<p>时段提名机制有许多，可以分为两类：一、前置类。在监督学习中，有的是使用actioness作为判断依据，再根据一些算法求得一段“预期有动作发生”的机制。二、后置类。即根据每帧的分类结果，再用一些算法求出一段某动作置信度都比较高的时间。</p>
<p>位置提名机制有许多。有的是在帧上，用物体检测的方法，如F-CNN等。有的是用attention机制。</p>
<p>Frame Embedding的产生，其实就是一个输入为帧级信息的网络。现在都用Two-stream作为输入。然后在网络结构上有各式各样的设计。</p>
<p>Video Embedding的产生，是依靠Frame Embedding的融合。有人用的是平均池化。有人用“时间金字塔”来融合。</p>
<p>分类机制。分两类：一、视频级分类。二、帧级分类。视频级分类的输入是Video Embedding。又或者是输入所有Frame Embedding给RNN。帧级分类则是每帧都进行一次分类。然后取最大子数组什么的。目前没有看到用CRF代替RNN进行帧级分类的文章。相信效果会比RNN好——显然，前一秒是某个动作的话，下一秒不可能突变到另一个动作。</p>
<h1 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h1><p>半监督学习指的是只有少部分数据有Label。而大部分没有。</p>
<h2 id="GAN-Improved-Techniques-for-Training-GANs"><a href="#GAN-Improved-Techniques-for-Training-GANs" class="headerlink" title="(GAN) Improved Techniques for Training GANs"></a>(GAN) Improved Techniques for Training GANs</h2><p>提出一种通过扩增判别器的输出实现半监督学习的GAN方法。详见<strong>跨领域参考</strong>。</p>
<h2 id="ML-Semi-Supervised-Learning-with-Ladder-Networks"><a href="#ML-Semi-Supervised-Learning-with-Ladder-Networks" class="headerlink" title="(ML) Semi-Supervised Learning with Ladder Networks"></a>(ML) Semi-Supervised Learning with Ladder Networks</h2><p>详见<strong>跨领域参考</strong>。</p>
<h2 id="semi-Supervised-Transfer-learning-Domain-Adversarial-Training-of-Neural-Networks"><a href="#semi-Supervised-Transfer-learning-Domain-Adversarial-Training-of-Neural-Networks" class="headerlink" title="(semi-Supervised, Transfer learning) Domain-Adversarial Training of Neural Networks"></a>(semi-Supervised, Transfer learning) Domain-Adversarial Training of Neural Networks</h2><p>详见<strong>跨领域参考</strong>。</p>
<h2 id="ML-Distributional-Smoothing-with-Virtual-Adversarial-Training"><a href="#ML-Distributional-Smoothing-with-Virtual-Adversarial-Training" class="headerlink" title="(ML) Distributional Smoothing with Virtual Adversarial Training"></a>(ML) Distributional Smoothing with Virtual Adversarial Training</h2><p>详见<strong>跨领域参考</strong>。</p>
<h1 id="弱监督学习"><a href="#弱监督学习" class="headerlink" title="弱监督学习"></a>弱监督学习</h1><p>弱监督学习的label只有动作分类，而没有详细的动作开始和结束的时间。</p>
<p><strong>！注意：在动作检测领域，有些数据集甚至是这样的：有所有动作的标签，但是只有一部分样本有时间定位。这其实是半监督学习和弱监督学习都有的情况。</strong></p>
<h2 id="Weakly-Supervised-Action-Localization-by-Sparse-Temporal-Pooling-Network"><a href="#Weakly-Supervised-Action-Localization-by-Sparse-Temporal-Pooling-Network" class="headerlink" title="Weakly Supervised Action Localization by Sparse Temporal Pooling Network"></a>Weakly Supervised Action Localization by Sparse Temporal Pooling Network</h2><p><img src="/Users/pengzhenghao/Documents/hexo/source/images/20180526/weakly20182.png" alt="网络结构"><br><img src="/Users/pengzhenghao/Documents/hexo/source/images/20180526/weakly2018.png" alt="网络结构"><br>帧级网络：“Quovadis,actionrecognition? a new model and the kinetics dataset”提供的</p>
<p>添加了一个权重单元，从而使得在生成Video Representation的时候，加权地pooling不同帧的Representation。注意这个权重单元是“动作无关”的。</p>
<p>由“ Learning deep features for discriminative localization”启发设计了Temporal Class Activation Mapping(T-CAM)。其实是利用了分类器的激活值，来知道<br>“这一刻，此动作有没有在发生”。</p>
<p>时段proposal机制：利用T-CAM，经过threshold，得到的许多许多段视频。就完了。我个人觉得如果用了CUHK的那个水淹算法可能会更好。</p>
<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><h2 id="Quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset"><a href="#Quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset" class="headerlink" title="Quo vadis, action recognition? a new model and the kinetics dataset"></a>Quo vadis, action recognition? a new model and the kinetics dataset</h2><p>提出了kinetics数据集，用来做迁移学习效果极佳。没有细看。</p>
<p><strong>这篇文章应该对我们选取数据集有很大帮助。</strong></p>
<h2 id="Multi-region-Two-Stream-R-CNN-for-Action-Detection"><a href="#Multi-region-Two-Stream-R-CNN-for-Action-Detection" class="headerlink" title="Multi-region Two-Stream R-CNN for Action Detection"></a>Multi-region Two-Stream R-CNN for Action Detection</h2><p>彭老师的一篇文章。构造了一个完整的时空监测与定位系统。代码极其纷繁复杂。但是思路非常清晰。简而言之就是用帧级网络提供动作分类和BBox，而后用经典的最大子数组算法得到动作的起止时间。</p>
<p>首先，提出了一个专门适配于视频的Faster R-CNN。FRCNN其实就是一堆CNN加上一个精心设计的Region proposal network。在这里用了两个FRCNN，一个负责单帧的RGB帧，这对应于传统的物体检测模型，提供（类似于）people proposal。另一个RCNN的输入是Stacked的光流图。用到了给定时刻前后几针的光流，提供motion proposal。随后，给出的一堆RoI会经过一个最大值池化层，然后传给后续的网络进行分类。</p>
<p>其次，提出了一种增加动作检测准确度的方法。最大值池化层之后，本来可以运用简单的FCN或CNN做分类。但是，这里提出了Multi-region思想，将给出的一个RoI扩展为原、上、下、边缘四个RoI，分别提交CNN去做分类，最后将四组softmax输出做求和，得到最终的分类结果。</p>
<p>最后，给了一种从帧级网络扩展到视频级网络的方法。刚才不是得到了每个帧下的分类结果嘛，那简单的使用最大子数组算法求得开始起止就行了。</p>
<p>优点：思路清晰，很精致。</p>
<p>缺点：帧级网络，没有考虑画面的时间结构。最大子数组算法可能对于异常帧失效。监督的。</p>
<h2 id="Temporal-Action-Detection-with-Structured-Segment-Networks"><a href="#Temporal-Action-Detection-with-Structured-Segment-Networks" class="headerlink" title="Temporal Action Detection with Structured Segment Networks"></a>Temporal Action Detection with Structured Segment Networks</h2><p>CUHK的一篇好文章。把图像识别的特征金字塔思想，变成了视频中的时序金字塔思想。</p>
<p>先用一个Naive的“动作性”（actioness）回归器为每帧给出动作性。然后使用classic watershed algorithm（类似于最大子数组？）来得到一个proposal。问题在于需要提供一个水位因子 $\gamma$，和融合小时段的一个因子$\tau$（类似于Photoshop中的扩大选区）。</p>
<p>之后把时段Proposal向前扩展一半，向后扩展一半，得到三个时段：前中后。对中间时段将会执行时序金字塔。</p>
<p>这一块要掰开说，一方面，所谓的时序金字塔，是指构造几个层级的时段长度，比如中间时段原来是10s，那么可以构造两个层级，第一个层级包含一个10s的视频，另一个层级包含两端原来那个10s的视频拆成的两个5s的视频。换句话说，现在有三个视频了。将这三个视频传给三个神经网络，最终得到三个视频的representation（embedding）。另一方面，到底怎么把视频encode成embedding呢？答案是采取传统的帧级神经网络，这个神经网络自然也是two-stream的（光流+RGB）。</p>
<p>前中后三个时段，考虑对中段施加了一个二级金字塔，总共有5个短视频的embedding。这五个东西concat一下传给输出层。输出层包含一堆分类器。其中有一个动作检测分类器，自然是多分类器。另外还有K个（动作类别个）二分类器，来决定completeness完成度。完成度指代的是：一个时段proposal是否包含了一个<strong>完整</strong>的动作。</p>
<p>优点：设计精巧。尤其是时间金字塔的设计。</p>
<p>缺点：</p>
<ul>
<li>彭老师评论：“ssn现在没什么了”。</li>
<li>将帧级网络的输出直接取平均值真的OK？一个proposal一百多秒你就取了个平均值？</li>
<li>监督的。</li>
</ul>
<h2 id="RPAN：An-end-to-end-recurrent-pose-attention-network-for-action-recognition"><a href="#RPAN：An-end-to-end-recurrent-pose-attention-network-for-action-recognition" class="headerlink" title="RPAN：An end-to-end recurrent pose-attention network for action recognition"></a>RPAN：An end-to-end recurrent pose-attention network for action recognition</h2><p>乔老师的文章。视频级的分类。</p>
<p>利用了Joint location标注，去训练了一个中间的attention层，从而与“直接将一个视频扔给CNN+LSTM做分类”比更高的准确度。但是需要Joint location标注。而且没有给出temporal proposal。</p>
<h2 id="Online-RED-Reinforced-Encoder-Decoder-Networks-for-Action-Anticipation"><a href="#Online-RED-Reinforced-Encoder-Decoder-Networks-for-Action-Anticipation" class="headerlink" title="(Online) RED: Reinforced Encoder-Decoder Networks for Action Anticipation"></a>(Online) RED: Reinforced Encoder-Decoder Networks for Action Anticipation</h2><p>2017年7月的文章。用到了强化学习。</p>
<p>过去的方法要不是利用LSTM读取旧视频的信息给出分类，要不就是用前一帧去生成下一帧的视频的Frame embedding，然后在给这个Embedding分类。</p>
<ul>
<li>Video representation extractor: 两种：two-stream CNN（6帧光流CNN得到motion，最中间的一帧用CNN得到apperance），VGG-16（只用中间帧）</li>
<li>encoder-decoder network：Encoder是一个LSTM。last hidden state给Decoder。Decoder输出一组Frame representation。用平方误差来做loss。（<strong>疑惑：</strong>原来的Representation是包含了6帧信息的CNN的输出。你让一个Lstm来做不合理吧？而且人家是6帧合一啊，你这个一帧输出的算个什么东西呢？）</li>
<li>classification network：LSTM不是输出了许多帧嘛，每一帧做一次分类。然后Loss就把分类和ground truth的对数熵做两个求和：时间和类别，堆起来就好。</li>
<li>reinforcement module/policy gradient algorithm: 以feature vector作为observation，以分类作为action。得到一个J函数（训练Policy网络的那个常用函数啦）作为loss，加到总loss里。</li>
</ul>
<p>这里的RL组件，是一种非常奇怪的RL。我认为，可以直接用一个经过修改的Loss函数代替。</p>
<p>我这里说的这么绝对，似乎有被打脸的风险。但是我还是觉得这篇文章的用法完全不妥：RL讲究的是在当前状态下，你选取了某个动作，从而环境的状态从现在的状态转移到了另一个状态。如果状态的转移过程和你选了哪个动作没有关系的话，那么这个动作对应的reward就无从谈起了。但是在这里，Reward函数是设计的跟(state, action)有关了，但是跟next state无关。<strong>我认为这个地方的RL是个幌子，其本质只是在Loss函数中加了一个正则项。</strong></p>
<p>但是他的思想是值得肯定的，那就是Reward的设计，和“追求更早的发现异常”这个追求。</p>
<p>这篇文章作为难得的Online检测的文章，主要是设计了anticipation网络。其本质是用LSTM去模仿下一帧的representation。等于说生生造出了一个视频。然后用这个representation结合老方法去做classification。</p>
<p>本文的弊端还是很多的。第一是RL模块很迷。第二是这个网络既没有Action Region Proposal，也没有Start time proposal，也没有更多的可以挖掘的地方，比如说概率的累加问题、计算效率等等。</p>
<h1 id="跨领域参考"><a href="#跨领域参考" class="headerlink" title="跨领域参考"></a>跨领域参考</h1><h2 id="NLP-region-End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF"><a href="#NLP-region-End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF" class="headerlink" title="(NLP region) End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"></a>(NLP region) End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</h2><p>看到了CRF是个很合理的东西，想把它与NN结合，没想到早就在NLP领域有人用过了。我估计在视频领域也有人用过了，但是还没细找。现在先看看这篇。我主要关心：如何把CRF做成NN中的一个层，并可以E2E的训练。</p>
<p>思路：每个单词的的one-hot形式，可以被排列成一个2D图像。用CNN对这个图像做处理，得到一个Character-level Embedding（一个单词一个Ebd）。这个C-level Ebd和一个传统的Word Ebd向量Concat起来，传给biLstm。在这个任务中，在每给定一个word的输入时，这个word对应的两个biLSTM隐含状态被输入到CRF中。CRF在这里被视为一个普通的层。它的输入是一个序列，它的输出也是一个同样长度的序列。只不过它考虑了输出序列中每个时刻的分类的前后关系。因此是一个更高级的RNN。</p>
<p>文章中没有说怎么训练CRF。估计在NLP领域CRF已经是个很成熟的组件了。</p>
<h2 id="GAN-semi-Supervised-Improved-Techniques-for-Training-GANs"><a href="#GAN-semi-Supervised-Improved-Techniques-for-Training-GANs" class="headerlink" title="(GAN, semi-Supervised) Improved Techniques for Training GANs"></a>(GAN, semi-Supervised) Improved Techniques for Training GANs</h2><p>将GAN用在半监督领域。创造性的一种设计：让判别器输出K+1个类。其中最后一个类为判定图像为假。那么Loss写成</p>
<script type="math/tex; mode=display">L = L_{supervised} + L_{unsupervised}</script><script type="math/tex; mode=display">L_{supervised} = -E[\log p_{model}(y|x)]</script><script type="math/tex; mode=display">L_{unsupervised} = -\{E_{p_{data}}[\log [1 -  p_{model} (y=K+1|x) + E_{generated}[\log p_{model} (y=K+1|x)]\}</script><p>前一项是普通的分类误差，第二项是判别误差。<br>猪的注意的是将$D(x) = 1-p_{model} (y=K+1|x)$的话则</p>
<script type="math/tex; mode=display">L_{unsupervised} = -E_{x\sim p_{data}(x)}\log D(x) + E_{z\sim noise}\log (1-D(G(Z)))</script><p>正是GAN的形式。</p>
<p>补充：<a href="https://blog.csdn.net/qq_25737169/article/details/78532719" target="_blank" rel="noopener">网友</a>说：“在代码中我给出了两种损失函数，一个是原始GAN的交叉熵损失函数，和DCGAN使用的一样，另外一个是improved wgan论文中使用的损失函数，但是在做了对比之后，我强烈建议使用DCGAN来做，improved wgan的损失函数虽然在生成结果的优化上有很大帮助，但是并不适合半监督学习中。”</p>
<h2 id="semi-Supervised-Transfer-learning-Domain-Adversarial-Training-of-Neural-Networks-1"><a href="#semi-Supervised-Transfer-learning-Domain-Adversarial-Training-of-Neural-Networks-1" class="headerlink" title="(semi-Supervised, Transfer learning) Domain-Adversarial Training of Neural Networks"></a>(semi-Supervised, Transfer learning) Domain-Adversarial Training of Neural Networks</h2><p><img src="/Users/pengzhenghao/Documents/hexo/source/images/20180526/domain2016.png" alt=""><br>单输入多输出网络。对于有lebel的数据，最小化分类误差和域误差。对于无label的数据，最小化域误差。<br>所谓域误差，就是一个分类器，判定这个数据是来自有lebel那类的还是没label那类的。它的目的，是要调整网络参数，使得网络的隐含层对输入数据有无label不敏感，即将有label的数据和无label的数据混在一起。这其实利用了一个假设：有无label的数据的分布是一样的。那既然是一样的，就意味着虽然无label的数据没有label，但是它应该和同属于一类的有label的数据聚集在一起，所以如果利用域分类器来将它们混在一起，那么等于说网络从无label的分布中，领悟到了“到底啥样的数据应该要聚在一起”，即从无label的数据中学习到了数据的一些基本的结构，从而帮助了有监督学习。</p>
<h2 id="ML-Semi-Supervised-Learning-with-Ladder-Networks-1"><a href="#ML-Semi-Supervised-Learning-with-Ladder-Networks-1" class="headerlink" title="(ML) Semi-Supervised Learning with Ladder Networks"></a>(ML) Semi-Supervised Learning with Ladder Networks</h2><p><img src="/Users/pengzhenghao/Documents/hexo/source/images/20180526/semi2015.png" alt=""><br>我们知道，拿一个纯正的x去预测一个纯正的y，这是监督任务。<br>我们知道AutoEncoder可以拿一个x去形成representation，再reconstruct一个新x。输入的x甚至可以使Corrupted的。<br>我们还知道AE的loss是新x和输入x的差的一个范数。</p>
<p>如果把AE的思想和监督任务的思想结合起来，并且扩展AE的任务：<br>不仅要reconstruct输入，而且要reconstruct所有隐含层。并且增加一个原隐含层和打算重构的隐含层之间的短路连接。那这就是一个很复杂的AE。注意有三个$z$，一个是干净的x前馈后得到的$z$，一个是有噪音的x前馈后得到的$\tilde{z}$，一个是由输入不干净的x的重构层的$\hat{z}$。</p>
<script type="math/tex; mode=display">Reconstruct Loss = || z^{(l)} - \hat{z}^{(l)}||</script><p>如果我们再在中间层加一个岔口，叉出去后加一个激活函数层变成了最终label的输出y，那么就可以把有监督任务和无监督任务放在一起变成半监督任务了。</p>
<p>这篇论文的改进就是在 Ladder Networks 上，encoder 部分的每一层 layer 都加入了 Gaussian noise，并保持 decoder 部分是 noise-free 的。加了 noise 的部分用于 unsupervised autoencoder loss，noise-free 的就是用来提供 supervised loss。</p>
<h2 id="ML-Distributional-Smoothing-with-Virtual-Adversarial-Training-1"><a href="#ML-Distributional-Smoothing-with-Virtual-Adversarial-Training-1" class="headerlink" title="(ML) Distributional Smoothing with Virtual Adversarial Training"></a>(ML) Distributional Smoothing with Virtual Adversarial Training</h2><p>给一个x，它的标签已知，是y。如果我们发现在x旁边一点点的一个地方一个x’，模型预测出来的标签却不是y。那么，这就违背了一个假设：临近的样本一定有临近的标签。</p>
<p>把这个情况泛化一下，令：</p>
<script type="math/tex; mode=display">\Delta_{KL} (r, x^{(n)}, \theta) = KL\{p(y|x^{(n)},\theta)\ ||\ p(y|x^{(n)}+r, \theta)\}</script><script type="math/tex; mode=display">r^{(n)}_{v-adv} = \mathop{argmax}_{r} \{\Delta_{KL}(r, x^{(n)}, \theta)\},\ ||r||_2 \leq \epsilon</script><p>上式用KL散度刻画了样本点x和样本点附近一点点的x+r对应的y的后验分布的散度。换言之就是刻画了这两个样本点之间的y是否相似。然后找到“最不相似”的那个x+r，强制要求Loss函数去优化这个“最不相似”的x+r处的y的分布情况。</p>
<script type="math/tex; mode=display">Loss_{LDS} = \cfrac{1}{N}\sum^{N}_{n=1} -\Delta_{KL}(r^{(n)}_{v-adv}, x^{(n)}, \theta)</script><p>由于我们虚拟了一个样本$x+r_{v-adv}$，所以叫VAT。另外上面的LDS，指的就是Local distributional smoothing。可以认为是一种正则化方法，将分布更均匀化了。</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>请我喝杯咖啡吧！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat.png" alt="彭正皓 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/ali.jpeg" alt="彭正皓 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/26/20180526art/" rel="next" title="资本主义的周期律及续命方式综述">
                <i class="fa fa-chevron-left"></i> 资本主义的周期律及续命方式综述
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNjgzOC8xMzM3NA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="彭正皓" />
            
              <p class="site-author-name" itemprop="name">彭正皓</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/pengzhenghao" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:pengzh@sjtu.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.douban.com/people/146604799/" target="_blank" title="豆瓣">
                      
                        <i class="fa fa-fw fa-globe"></i>豆瓣</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/peng-zhenghao/" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-globe"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#半监督学习"><span class="nav-text">半监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GAN-Improved-Techniques-for-Training-GANs"><span class="nav-text">(GAN) Improved Techniques for Training GANs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ML-Semi-Supervised-Learning-with-Ladder-Networks"><span class="nav-text">(ML) Semi-Supervised Learning with Ladder Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#semi-Supervised-Transfer-learning-Domain-Adversarial-Training-of-Neural-Networks"><span class="nav-text">(semi-Supervised, Transfer learning) Domain-Adversarial Training of Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ML-Distributional-Smoothing-with-Virtual-Adversarial-Training"><span class="nav-text">(ML) Distributional Smoothing with Virtual Adversarial Training</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#弱监督学习"><span class="nav-text">弱监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Weakly-Supervised-Action-Localization-by-Sparse-Temporal-Pooling-Network"><span class="nav-text">Weakly Supervised Action Localization by Sparse Temporal Pooling Network</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#监督学习"><span class="nav-text">监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Quo-vadis-action-recognition-a-new-model-and-the-kinetics-dataset"><span class="nav-text">Quo vadis, action recognition? a new model and the kinetics dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-region-Two-Stream-R-CNN-for-Action-Detection"><span class="nav-text">Multi-region Two-Stream R-CNN for Action Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Temporal-Action-Detection-with-Structured-Segment-Networks"><span class="nav-text">Temporal Action Detection with Structured Segment Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RPAN：An-end-to-end-recurrent-pose-attention-network-for-action-recognition"><span class="nav-text">RPAN：An end-to-end recurrent pose-attention network for action recognition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Online-RED-Reinforced-Encoder-Decoder-Networks-for-Action-Anticipation"><span class="nav-text">(Online) RED: Reinforced Encoder-Decoder Networks for Action Anticipation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#跨领域参考"><span class="nav-text">跨领域参考</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP-region-End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF"><span class="nav-text">(NLP region) End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GAN-semi-Supervised-Improved-Techniques-for-Training-GANs"><span class="nav-text">(GAN, semi-Supervised) Improved Techniques for Training GANs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#semi-Supervised-Transfer-learning-Domain-Adversarial-Training-of-Neural-Networks-1"><span class="nav-text">(semi-Supervised, Transfer learning) Domain-Adversarial Training of Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ML-Semi-Supervised-Learning-with-Ladder-Networks-1"><span class="nav-text">(ML) Semi-Supervised Learning with Ladder Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ML-Distributional-Smoothing-with-Virtual-Adversarial-Training-1"><span class="nav-text">(ML) Distributional Smoothing with Virtual Adversarial Training</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">彭正皓</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
